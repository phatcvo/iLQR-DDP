{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class Env:\n",
    "    def __init__(self, rows, columns):      \n",
    "        self.rows = rows                                    # Number of rows of gridworld\n",
    "        self.columns = columns                              # Number of columns of gridworld\n",
    "        self.num_states = rows*columns                      # Number of states\n",
    "        self.actions = self.define_actions()                # Define the actions the agent can take\n",
    "        self.num_actions = len(self.actions)                # Number of actions the agent can take\n",
    "        self.rewards = self.define_rewards()                # Define the reward for each state\n",
    "        self.obstacles = self.define_obstacles()            # Define the obstacles of the environment\n",
    "        self.states = [(i,j) for i in range(rows) for j in range(columns)]  # State coordinates   \n",
    "        self.directions = ['front', 'to_left', 'to_right']  # Non deterministic dynamics\n",
    "        self.distribution = [.8, .1, .1]                    # Probability distribution of dynamics            \n",
    "    \n",
    "    def define_actions(self):\n",
    "        \"\"\"Define the possible actions that the agent can take.\"\"\"\n",
    "        actions = ['up', 'right', 'down', 'left']  # 0 = up, 1 = right, 2 = down, 3 = left       \n",
    "        return actions   \n",
    "    \n",
    "    def define_rewards(self):\n",
    "        \"\"\"Define the reward for each state.\"\"\"\n",
    "        rewards = -0.04*np.ones([rows, columns])\n",
    "        rewards[0, 3] = 1.0\n",
    "        rewards[1, 3] = -1.0      \n",
    "        return rewards\n",
    "    \n",
    "    def define_obstacles(self):\n",
    "        \"\"\"Define the obstacles of the environment and set their rewards to None.\"\"\"\n",
    "        obstacles = [(1,1)]           # Locations of obstacles\n",
    "        for obs in obstacles:\n",
    "            self.rewards[obs] = None  # Set reward of obstacle locations to None\n",
    "        return obstacles\n",
    "\n",
    "    def is_terminal_state(self, state):\n",
    "        \"\"\"Determines whether the state is terminal. \n",
    "        \n",
    "        Inputs:\n",
    "          - state(tuple): A state of the environment\n",
    "        Returns:\n",
    "          - (bool):       Whether the state is terminal or not\n",
    "        \"\"\"    \n",
    "\n",
    "        if self.rewards[state] == -1.0 or self.rewards[state] == 1.0:\n",
    "            return True \n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"Get the next state based on the current state and the chosen action.\n",
    "        \n",
    "        The dynamics are non-deterministic and the agent transitions to the desired \n",
    "        state with a high probability but might slip to the left or to the right.\n",
    "        Inputs:\n",
    "          - state(tuple): The current state of the agent\n",
    "          - action(int):  The chosen action to take\n",
    "        Returns:\n",
    "          - (tuple):      The next state of the agent\n",
    "        \"\"\"\n",
    "        action_word = self.actions[action]  # Get the action in text form\n",
    "\n",
    "        # Adjust action if agent 'slips'\n",
    "        non_determ_direction = random.choices(self.directions, self.distribution)\n",
    "        if non_determ_direction == 'to_left':\n",
    "            if action_word == 'up':\n",
    "                action_word = 'left'\n",
    "            elif action_word == 'right':\n",
    "                action_word = 'up'\n",
    "            elif action_word == 'down':\n",
    "                action_word = 'right'\n",
    "            elif action_word == 'left':\n",
    "                action_word = 'down'\n",
    "        elif non_determ_direction == 'to_right':\n",
    "            if action_word == 'up':\n",
    "                action_word = 'right'\n",
    "            elif action_word == 'right':\n",
    "                action_word = 'down'\n",
    "            elif action_word == 'down':\n",
    "                action_word = 'left'\n",
    "            elif action_word == 'left':\n",
    "                action_word = 'up'\n",
    "\n",
    "        # Make permissible movement\n",
    "        row, column = state\n",
    "        if action_word == 'up' and row > 0 and ((row-1, column) not in self.obstacles):        \n",
    "            row -= 1     # If agent is not at the upper wall or at obstacle, move up.\n",
    "        elif action_word == 'right' and column < self.columns - 1 and ((row, column+1) not in self.obstacles):\n",
    "            column += 1  # If agent is not at the right wall or at obstacle, move right.\n",
    "        elif action_word == 'down' and row < self.rows - 1 and ((row+1, column) not in self.obstacles):\n",
    "            row += 1     # If agent is not at the bottom wall or at obstacle, move down.\n",
    "        elif action_word == 'left' and column > 0 and ((row, column-1) not in self.obstacles):\n",
    "            column -= 1  # If agent is not at the left wall or at obstacle, move left.\n",
    "\n",
    "        return (row, column)  # Next state \n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, start_state, a, epsilon): \n",
    "        self.env = env                        # The environment class\n",
    "        self.num_actions = env.num_actions    # Number of actions the agent can take\n",
    "        self.start_state = start_state        # The state that the agent begins in\n",
    "        self.epsilon = epsilon                # Exploration probability\n",
    "        self.q_table = self.define_q_table()  # The Q-table\n",
    "        self.a = a                            # Learning rate        \n",
    "\n",
    "    def define_q_table(self):\n",
    "        \"\"\"Initializes the Q-table arbitrarily except Q(terminal,.)=0.\"\"\"\n",
    "        q_table = np.random.uniform(low=0.01, high=1.0, size=(self.env.num_states, self.num_actions))\n",
    "        \n",
    "        for state in self.env.states:\n",
    "            if self.env.is_terminal_state(state):\n",
    "                state_idx = self.env.states.index(state)                \n",
    "                q_table[state_idx, :] = 0  \n",
    "                \n",
    "        return q_table\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose next action to take from current state with epsilon-greedy policy.\n",
    "\n",
    "        Explore the environment with probability epsilon and\n",
    "        Exploit knowledge and take greedy action with probability (1-epsilon).\n",
    "        Inputs:\n",
    "          - state(tuple): The current state of the agent\n",
    "        Returns:\n",
    "          - action(int):  The next action to take\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.random() < self.epsilon:         \n",
    "            action = np.random.randint(self.num_actions)  # Explore environment      \n",
    "        else: \n",
    "            state_idx = self.env.states.index(state)      # State index\n",
    "            action = np.argmax(self.q_table[state_idx])   # Exploit learned values\n",
    "        return action    \n",
    "    \n",
    "    def choose_action_trained(self, state):\n",
    "        \"\"\"Choose the best next action to take from the current state.\n",
    "\n",
    "        Inputs:\n",
    "          - state(tuple): The current state of the agent\n",
    "        Returns:\n",
    "          - (int):        The next action to take\n",
    "        \"\"\"\n",
    "        state_idx = self.env.states.index(state)          # State index\n",
    "        return np.argmax(self.q_table[state_idx])         # Best action\n",
    "\n",
    "    def get_path(self, init_state):\n",
    "        \"\"\"Get the path from the initial state to a terminal state.\n",
    "        \n",
    "        Inputs:\n",
    "          - init_state(tuple): The initial state that the agent begins in\n",
    "        Returns:\n",
    "          - path(list):        The sequence of states the agent takes\n",
    "        \"\"\"\n",
    "\n",
    "        state = init_state\n",
    "        path = [state]\n",
    "        while not self.env.is_terminal_state(state):\n",
    "            action = self.choose_action_trained(state)            \n",
    "            state = self.env.get_next_state(state, action)\n",
    "            path.append(state)\n",
    "        return path\n",
    "    \n",
    "    def save_q_history(self, episode):\n",
    "        \"\"\"Save the values of the Q-table for plotting.\"\"\"\n",
    "        for state_idx in range(self.env.num_states):\n",
    "            for action in range(self.env.num_actions):\n",
    "                self.q_history[state_idx][action,episode] = self.q_table[state_idx, action]\n",
    "\n",
    "    def plot_q_history(self):\n",
    "        \"\"\"Plots the q-values for each state-action pair vs episodes.\"\"\"\n",
    "        print()\n",
    "        sns.set_theme()\n",
    "        for state_idx in range(self.env.num_states):  \n",
    "            if state_idx < 5:          \n",
    "                fig, axes = plt.subplots(1, 4, figsize=(15,4))\n",
    "                fig.suptitle('Q-values for state '+ str(state_idx))            \n",
    "                for action in range(self.env.num_actions):            \n",
    "                    axes[action].plot(self.q_history[state_idx][action])\n",
    "                    title = self.env.actions[action].capitalize()\n",
    "                    axes[action].set_title(title)\n",
    "                    axes[action].set_ylim([0, 1])\n",
    "                plt.show     \n",
    "            elif state_idx == 5:  pass\n",
    "            else:\n",
    "                fig, axes = plt.subplots(1, 4, figsize=(15,4))\n",
    "                fig.suptitle('Q-values for state '+ str(state_idx-1))            \n",
    "                for action in range(self.env.num_actions):            \n",
    "                    axes[action].plot(self.q_history[state_idx][action])\n",
    "                    title = self.env.actions[action].capitalize()\n",
    "                    axes[action].set_title(title)\n",
    "                    axes[action].set_ylim([0, 1])\n",
    "                plt.show     \n",
    "        return\n",
    "    \n",
    "    def train(self, init_state, episodes):\n",
    "        \"\"\"Trains the agent via Q-Learning.\n",
    "        \n",
    "        Updates the Q-table with the optimal policy.\n",
    "        Inputs:\n",
    "          - init_state(tuple): The initial state that the agent begins in\n",
    "          - episodes(int):     Number of episodes to train for\n",
    "        \"\"\"\n",
    "\n",
    "        gamma = 1.0      # Discount factor for future rewards        \n",
    "        self.q_history = {i: np.zeros((4,episodes)) for i in range(self.env.num_states)}  # Initialize dict to save q values history\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            state = init_state             # Initialize/reset state\n",
    "            self.save_q_history(episode)   # Save Q-values history\n",
    "\n",
    "            while not self.env.is_terminal_state(state):\n",
    "                \n",
    "                # Choose action to take\n",
    "                action = self.choose_action(state)  \n",
    "\n",
    "                # Take chosen action and transition to the next state \n",
    "                next_state = self.env.get_next_state(state, action)\n",
    "                \n",
    "                # Receive the reward \n",
    "                reward = self.env.rewards[next_state]                \n",
    "\n",
    "                # Update Q-value \n",
    "                state_idx = self.env.states.index(state)            # State index\n",
    "                next_state_idx = self.env.states.index(next_state)  # Next state index\n",
    "                q_value = self.q_table[state_idx, action]\n",
    "                self.q_table[state_idx, action] = q_value + self.a*(reward + gamma*np.max(self.q_table[next_state_idx]) - q_value)\n",
    "                \n",
    "                # Update state\n",
    "                state = next_state\n",
    "            \n",
    "        print('Training complete!\\n')\n",
    "\n",
    "\n",
    "def plot_path(path, q_table, env):\n",
    "    \"\"\"Plots the optimal path in the gridworld environment.\n",
    "    \n",
    "    Inputs:\n",
    "      - path(list):          The optimal path\n",
    "      - q_table(np.ndarray): The Q-table\n",
    "      - env(class.Env):      The environment\n",
    "    \"\"\"\n",
    "    rows = env.rows\n",
    "    columns = env.columns\n",
    "            \n",
    "    fig, ax = plt.subplots(figsize = (12,8))\n",
    "    ax.set_title('Gridworld')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Grid\n",
    "    for row in range(rows+1):\n",
    "        ax.hlines(y=row, xmin=0, xmax=columns, linewidth=1.5, color='k')    \n",
    "    for column in range(columns+1):\n",
    "        ax.vlines(x=column, ymin=0, ymax=rows, linewidth=1.5, color='k')\n",
    "    \n",
    "    # Obstacles\n",
    "    for obs in env.obstacles:\n",
    "        row, col = obs\n",
    "        ax.add_patch(Rectangle((col, rows-1-row), 1, 1, color='k'))\n",
    "\n",
    "    # Annotate terminal states (hard-coded)\n",
    "    ax.annotate('+1', xy=(3.35, 2.4),color='#d42436', size='30')\n",
    "    ax.annotate('-1', xy=(3.4, 1.4),color='#d42436', size='30')\n",
    "\n",
    "    # Show path\n",
    "    for state in path:\n",
    "        row, col = state\n",
    "        ax.add_patch(Rectangle((col, rows-1-row), 1, 1, color=\"#104fb5\", alpha=0.2))\n",
    "    \n",
    "    # Optimal Policy (best action arrows)\n",
    "    optimal_actions = [np.argmax(row) for row in q_table]\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):   \n",
    "\n",
    "            state_idx = env.states.index((i, j))   # State index\n",
    "            if optimal_actions[state_idx] == 0:    # up\n",
    "                dx, dy = 0, 0.2\n",
    "            elif optimal_actions[state_idx] == 1:  # right\n",
    "                dx, dy = 0.2, 0\n",
    "            elif optimal_actions[state_idx] == 2:  # down\n",
    "                dx, dy = 0, -0.2\n",
    "            else:                                  # left\n",
    "                dx, dy = -0.2, 0\n",
    "            \n",
    "            if env.is_terminal_state((i, j)):\n",
    "                break  # If terminal state, don't draw arrow\n",
    "            plt.arrow(j+0.5, rows-1-i+0.5, dx, dy, head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "   \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_q_table(q_table, env):\n",
    "    \"\"\"Prints the Q-table in a more readable format and shows the state correspndaces.\"\"\"\n",
    "\n",
    "    print('\\t'*5 + 'Q-table \\n' + '-'*50)\n",
    "    print('\\t Up \\t\\t Right \\t\\t Down \\t Left\\n' + '-'*50 )    \n",
    "    rows, columns = q_table.shape\n",
    "    for i in range(rows):\n",
    "        if i < 5:\n",
    "            print(i, end=(2-len(str(i)))*' '+'|  ')\n",
    "            for j in range(columns):\n",
    "                print('{:10.4f}'.format(q_table[i, j]), end=' ')            \n",
    "        elif i < 5: pass  # Don't show results for obstacle state\n",
    "        else:\n",
    "            print(i-1, end=(2-len(str(i-1)))*' '+'|  ')\n",
    "            for j in range(columns):\n",
    "                print('{:10.4f}'.format(q_table[i, j]), end=' ')\n",
    "        print('\\n  |')\n",
    "    print('-'*50 + '\\n')\n",
    "\n",
    "    print('State Correspondence\\n'+20*'=')\n",
    "    print('State \\t Cell\\n'+20*'-')\n",
    "    for state in env.states:\n",
    "        if env.states.index(state) < 5:\n",
    "            print('{} \\t {}'.format(env.states.index(state), state))\n",
    "        elif env.states.index(state) == 5:\n",
    "            pass\n",
    "        else:\n",
    "            print('{} \\t {}'.format(env.states.index(state)-1, state))\n",
    "    print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    np.random.seed(0)    # To make runs repeatable\n",
    "\n",
    "    # Parameters\n",
    "    # -------------------------------------------------------------------------\n",
    "    rows = 3             # Number of rows of the gridworld \n",
    "    columns = 4          # Number of columns of the gridworld\n",
    "    start_state = (2,0)  # State the agent begins in\n",
    "    eps = 0.1            # Exploration probability\n",
    "    N = 20000            # Number of episodes to train for\n",
    "    a = 0.1              # Learning rate\n",
    "\n",
    "    # Train the agent\n",
    "    # -------------------------------------------------------------------------\n",
    "    env = Env(rows, columns)                                   # Define environment\n",
    "    agent = Agent(env, start_state, a, eps)                    # Define agent\n",
    "    agent.train(start_state, N)                                # Train the AI agent\n",
    "            \n",
    "    # Display results\n",
    "    # -------------------------------------------------------------------------\n",
    "    show_q_table(agent.q_table, env)                           # Display final Q-table\n",
    "    optimal_path = agent.get_path(start_state)                 # Find optimal path\n",
    "    print('The optimal path is: \\n{}\\n'.format(optimal_path))  # Display optimal path       \n",
    "\n",
    "    # Plot the optimal path in the gridworld environment\n",
    "    plot_path(optimal_path, agent.q_table, env)  \n",
    "\n",
    "    agent.plot_q_history() \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
